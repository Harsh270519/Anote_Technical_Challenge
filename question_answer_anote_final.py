# -*- coding: utf-8 -*-
"""Question_Answer_Anote_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GYmndEsWw175bSVHPwA3IGu4vzB2a8lV
"""

!pip install datasets

from google.colab import drive
drive.mount("/content/drive")

import pandas as pd

from datasets import load_dataset

dataset_1 = load_dataset("PatronusAI/financebench")

dataset_1

df = pd.DataFrame(dataset_1['train'])

selected_columns = df[['question', 'answer', 'evidence_text']]

selected_columns

selected_columns.rename(columns = {'evidence_text':'context'}, inplace = True)
selected_columns

dataset_2 = load_dataset("llmware/rag_instruct_benchmark_tester")

dataset_2

df_2 = pd.DataFrame(dataset_2['train'])

selected_columns_2 = df_2[['query', 'answer', 'context']]

selected_columns_2

selected_columns_2.rename(columns = {'query':'question'}, inplace = True)
selected_columns_2

frames = [selected_columns, selected_columns_2]

result = pd.concat(frames)

print(result)

result.head()

result.to_csv("/content/drive/MyDrive/anote/result.csv")

result_final = load_dataset('csv', data_files='/content/drive/MyDrive/anote/result.csv')
result_final

from sklearn.model_selection import train_test_split

train_df, val_df = train_test_split(result_final['train'], test_size=0.2, random_state=42)

print(type(train_df))
print(type(val_df))

!pip install -qU bitsandbytes datasets accelerate loralib transformers peft

import torch
torch.cuda.is_available()

!huggingface-cli login

!huggingface-cli whoami

import os
os.environ["CUDA_VISIBLE_DEVICES"]="0"
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig

model_id = "mistralai/Mistral-7B-Instruct-v0.2"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

base_model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    use_cache=False,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

print(base_model)

def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
    )

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

lora_config = LoraConfig(
    r=64,
    lora_alpha=128,
    #target_modules=["q_proj", "v_proj", "k_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

base_model = prepare_model_for_kbit_training(base_model)
model = get_peft_model(base_model, lora_config)
print_trainable_parameters(model)

print(model)

def generate_prompt(example, return_response=True) -> str:
    full_prompt = f"[CONTEXT]: {example['context']} [QUESTION]: {example['question']}"

    if return_response:
        full_prompt += f" [ANSWER]: {example['answer']}"
    return full_prompt

train_prompts = [generate_prompt({'question': q, 'answer': a, 'context': c}, return_response=True)
                 for q, a, c in zip(train_df['question'], train_df['answer'], train_df['context'])]
val_prompts = [generate_prompt({'question': q, 'answer': a, 'context': c}, return_response=True)
               for q, a, c in zip(val_df['question'], val_df['answer'], val_df['context'])]

train_encodings = tokenizer(train_prompts, truncation=True, padding=True, max_length=2048, return_tensors="pt")
val_encodings = tokenizer(val_prompts, truncation=True, padding=True, max_length=2048, return_tensors="pt")

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="mistral-7b-instruct",
    num_train_epochs=0.1,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=2,
    gradient_checkpointing=True,
    optim="paged_adamw_32bit",
    logging_steps=1,
    save_strategy="epoch",
    learning_rate=2e-4,
    max_grad_norm=0.3,
    warmup_ratio=0.03,
    lr_scheduler_type="constant",
    disable_tqdm=True
)

!pip install trl -U -q

from torch.utils.data import Dataset, DataLoader
import torch

class TextDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):

        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        return item

    def __len__(self):
        return len(self.encodings.input_ids)


train_dataset = TextDataset(train_encodings)
val_dataset = TextDataset(val_encodings)

from trl import SFTTrainer

trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    peft_config=lora_config,
    max_seq_length=2048,
    formatting_func=generate_prompt,
    tokenizer=tokenizer,
)

trainer.train()

trainer.save_model()

from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    training_args.output_dir,
    low_cpu_mem_usage=True,
    torch_dtype=torch.float16,
    load_in_4bit=True,
)
tokenizer = AutoTokenizer.from_pretrained(training_args.output_dir)

import json

def format_json_context(json_context):
    context_data = json.loads(json_context)

    def format_element(data, indent=0):
        lines = []
        if isinstance(data, dict):
            for key, value in data.items():
                lines.append(" " * indent + f"{key}:")
                lines.extend(format_element(value, indent + 2))
        elif isinstance(data, list):
            for i, item in enumerate(data):
                lines.append(" " * indent + f"- Item {i + 1}:")
                lines.extend(format_element(item, indent + 2))
        else:
            lines.append(" " * indent + str(data))
        return lines

    return "\n".join(format_element(context_data))

def process_codefinqa(model, tokenizer, question, context):
    system_message = "You are a document QA bot. You compute values from a given document context. To do so, first work through an answer step-by-step in natural language and then write a single bit of python code to compute the final solution. The python code cannot import external libraries or print any values. The implicit return value from the last statement will be provided as the answer. The answer should always be a single float value. Put the code in a markdown code block. (```python)"
    prompt = f"Context: {context} Question: {question} \n\n###\n\n"
    input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=2048).input_ids.cuda()
    output = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9, temperature=0.5)
    answer = tokenizer.decode(output[0], skip_special_tokens=True)
    return system_message + f"\n\n```python\n{answer}\n```"

def process_convfinqa(model, tokenizer, question, context):
    system_message = "You are a document QA bot. You return values from a given document context. You respond to the question by first thinking through the question step-by-step, and then provide with the final solution as [[NUMBER]]. Do not include any other text within the [[]], it should be a single number that could be directly cast into a python float. Make sure the number in the brackets matches the requested units. Any response without a [[]] is invalid."
    prompt = f"Context: {context} Question: {question}"
    input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=2048).input_ids.cuda()
    output = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9, temperature=0.5)
    answer = tokenizer.decode(output[0], skip_special_tokens=True)
    return system_message + f"\n\n[[{answer}]]"

def process_fincode_code(model, tokenizer, question, context):
    system_message = "You are a QA bot. You compute values to answer a given question. To do so, first work through an answer step-by-step in natural language and then write a single bit of python code to compute the final solution. The python code cannot import external libraries or print any values. The implicit return value from the last statement will be provided as the answer. The answer should always be a single float value. Put the code in a markdown code block. (```python)"
    prompt = f"Context: {context} Question: {question} \n\n###\n\n"
    input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=2048).input_ids.cuda()
    output = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9, temperature=0.5)
    answer = tokenizer.decode(output[0], skip_special_tokens=True)
    return system_message + f"\n\n```python\n{answer}\n```"

def process_finknow(model, tokenizer, question, context, options):
    system_message = "You are a multiple choice bot. You respond to the question by first thinking through the questions and the possible answers, and then respond with the final choice in as [[LETTER]]. For example, if the options are A, B and C and the answer is A, the final part of your message should be [[A]]. Do not include any other text within the [[]]"
    prompt = f"Context: {context} Question: {question} Options: {options}"
    input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=2048).input_ids.cuda()
    output = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9, temperature=0.5)
    answer = tokenizer.decode(output[0], skip_special_tokens=True)
    return system_message + f"\n\n[[{answer}]]"

def process_tatqa(model, tokenizer, question, context):
    system_message = "You are a document QA bot. You return values from a given document context. You respond to the question by first thinking through the question step-by-step, and then provide with the final solution as [[NUMBER]]. Do not include any other text within the [[]], it should be a single number that could be directly cast into a python float. Make sure the number in the brackets matches the requested units. Any response without a [[]] is invalid."
    prompt = f"Context: {context} Question: {question}"
    input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=2048).input_ids.cuda()
    output = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9, temperature=0.5)
    answer = tokenizer.decode(output[0], skip_special_tokens=True)
    return system_message + f"\n\n[[{answer}]]"

def process_codetatqa(model, tokenizer, question, json_context):
    system_message = "You are a document QA bot. You return values from a given document context. You respond to the question by first thinking through the question step-by-step, and then provide with the final solution as [[NUMBER]]. Do not include any other text within the [[]], it should be a single number that could be directly cast into a python float. Any response without a [[]] is invalid."
    formatted_context = format_json_context(json_context)
    prompt = f"Context: {formatted_context} Question: {question}"
    input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=2048).input_ids.cuda()
    output = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9, temperature=0.5)
    answer = tokenizer.decode(output[0], skip_special_tokens=True)
    return system_message + f"\n\n[[{answer}]]"

import pandas as pd

test_df = pd.read_excel('/content/drive/MyDrive/anote/TestingData GSheets.xlsx')

results = []

for index, row in test_df.iterrows():
    task_function_map = {
        'CodeFinQA': process_codefinqa,
        'ConvFinQA': process_convfinqa,
        'FinCode': process_fincode_code,
        'FinKnow': process_finknow,
        'CodeTAT-QA': process_codetatqa,
        'TAT-QA': process_tatqa
    }

    process_function = task_function_map.get(row['task'])

    if process_function:
        if row['task'] == 'FinKnow':
            answer = process_function(model, tokenizer, row['question'], row['context'], row['options'])
        elif row['task'] == 'CodeTAT-QA':
            answer = process_function(model, tokenizer, row['question'], row['context'])
        else:
            answer = process_function(model, tokenizer, row['question'], row['context'])
    else:
        answer = "Task not recognized"

    results.append({'id': row['id'], 'answer': answer})

results_df = pd.DataFrame(results)
results_df.to_csv("inference_results.csv", index=False)

results_df.to_csv("/content/drive/MyDrive/anote/inference_results.csv", index=False)

